---
title: "Black Sea currents"
date: "2018-07-03"
output: html_notebook
theme: paper
---

##  Black Sea circulation  
This notebook details the process of opening a NetCDF dataset downloaded from CMEMS (marine.copernicus.eu), extracting & then plotting the data.  

The current dataset contains a subset of surface current velocities from BLKSEA_REANALYSIS_PHYS_007_004 - a reanalysis product of the physical state of the Black Sea (1992-2017). The subset covers the SW Black Sea - Burgas Bay to the Turkish border, and the period 2012-01-01 to 2015-01-01 (my PhD study period).  

***  

Setup!  
```{r setup, include = FALSE}
library(knitr)

knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar = c(2, 2, .1, 2))  # smaller margin on top
})

## set the working directory to one directory up (I'm keeping the notebooks in their own subdirectory of the project, doc).
opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

## set knitr options for knitting code into the report.
opts_chunk$set(cache = TRUE, # save results so that code blocks aren't re-run unless code changes
               autodep = TRUE, # ..or unless a relevant earlier code block changed
               cache.comments = FALSE, # don't re-run if the only thing that changed was the comments
               highlight = TRUE, 
               small.mar = TRUE)

```

Define working subdirectories. 
```{r workspace_setup}
## print the working directory, just to be on the safe side
paste("You are here: ", getwd())

data.dir <- "data"    # input data files
functions.dir <- "R"  # functions & scripts
save.dir <- "output"  # clean data, output from models & more complex calculations
figures.dir <- "figs" # plots & figures 
```

Import libraries.
```{r import_packages, message = FALSE}
library(here) ## find root of project directory
library(tidyverse) ## data manipulation & tidying
library(viridis) ## pretty & readable colour schemes 
library(ncdf4) ## read & manipulate NetCDF files
library(maps) ## background maps
library(mapdata) ## background maps, too
library(rgdal) ## read & interact with shapefiles
library(sp) ## work with spatial data
```

Some commonly-used ggplot2 modifications..  
```{r custom_ggplot_settings_helpers}
## ggplot settings & things that I keep reusing
# ggplot_theme <- list(
#   theme_bw(),
#   theme(element_text(family = "Times"))
# )

## always use black-and-white theme
theme_set(theme_bw())

## helper to adjust ggplot text size & avoid repetitions 
text_size <- function(text.x = NULL,
                      text.y = NULL,
                      title.x = NULL,
                      title.y = NULL,
                      legend.text = NULL,
                      legend.title = NULL, 
                      strip.x = NULL, 
                      strip.y = NULL) {
  theme(axis.text.x = element_text(size = text.x),
        axis.text.y = element_text(size = text.y),
        axis.title.x = element_text(size = title.x),
        axis.title.y = element_text(size = title.y),
        legend.text = element_text(size = legend.text), 
        legend.title = element_text(size = legend.title), 
        strip.text.x = element_text(size = strip.x), 
        strip.text.y = element_text(size = strip.y)
        )
  }
```

***  

#### Import & clean current velocity data.  
First, the **daily means** of the current velocities.  
Read in the data & check the file contents.  
```{r import_netcdf}
(currents.in <- nc_open(file = here(data.dir, "currents", "sv04-bs-cmcc-cur-rean-d_1530092744997.nc"))
)
```

Looks fine! Let's extract the current velocities..  
```{r extract_netcdf_variables}
## meridional current matrix
currents.y <- ncvar_get(currents.in, "vomecrty")
dim(currents.y)

## zonal current matrix
currents.x <- ncvar_get(currents.in, "vozocrtx")
dim(currents.x)

```
The current velocities are stored in a lon x lat x time matrix. There is also a depth dimension, but it only has one value, 2.5 m - only surface currents here.  
Let's extract the time, latitude and longitude - stored as **NetCDF dimensions**.  
```{r extract_netcdf_dimensions}
## check the names of the dimensions 
attributes(currents.in$dim)$names

## we'll only get the latitude, longitude & time
nc_time <- ncvar_get(currents.in, "time")
nc_lat <- ncvar_get(currents.in, "lat")
nc_lon <- ncvar_get(currents.in, "lon")

## print the dimensions to see if they match what we have for the velocities above..
print(paste(dim(nc_lon), "longitudes, ", dim(nc_lat), "latitudes and ", dim(nc_time), "times."))
```

OK, there is no real need to get more of the attributes - let's close the NetCDF connection. 
```{r close_netcdf}
nc_close(currents.in)
```


Now for the cleaning...  
Reformat the time into something more readable (otherwise it's in seconds since 1990-01-01, as the attributes say).  
```{r format_time}
nc_time <- as.POSIXct(nc_time, format = "%Y-%m-%d", tz = "UTC", origin = "1990-01-01") ## the file metadata say that the time format is in seconds since 1990-01-01

head(nc_time)
```

OK; now collapse the array into a data frame which is infinitely easier to manage..
```{r arrays_to_data_frame}
## put the latitudes in column names, the longitudes in row names and the times in 3d dimension
dimnames(currents.x) <- list(lon = nc_lon, lat = nc_lat, time = nc_time)
dimnames(currents.y) <- list(lon = nc_lon, lat = nc_lat, time = nc_time)

## collapse the velocity arrays into data frames 
currents.x.df <- as.data.frame.table(currents.x, responseName = "u") 
currents.y.df <- as.data.frame.table(currents.y, responseName = "v")

## convert these data frames to tibbles, just because I like them better
currents.x.df <- as_data_frame(currents.x.df)
currents.y.df <- as_data_frame(currents.y.df)
```

Now, this turns the latitudes, longitudes and times into factors, and we don't want that.  
```{r fix_numeric_vars}
(currents.x.df <- currents.x.df %>% 
   ## convert factors to numeric
   mutate_if(is.factor, funs(as.numeric(as.character(.)))) %>% 
   ## fix time, KEEPING IN MIND THAT THIS TIME THE ORIGIN IS 1970-01-01, AS IN R
   mutate(time = as.POSIXct(time, tz = "UTC", origin = "1970-01-01"))
)


(currents.y.df <- currents.y.df %>% 
   ## convert factors to numeric
   mutate_if(is.factor, funs(as.numeric(as.character(.)))) %>% 
   mutate(time = as.POSIXct(time, tz = "UTC", origin = "1970-01-01"))
)
```
Now combine the two data frames (u and v) into a single large data frame..  
```{r combine_currents_df}
(currents.all <- left_join(currents.x.df, currents.y.df, by = c("lat", "lon", "time"))
)
```


#### Map data  
I'm going to play around - for practice - with various ways to get a background map, and see which one I like the most.. 

* use ggplot function map_data to get the desired area from the mapdata package database. Bonus: already converted to a data frame ggplot can read!  
```{r background_map_ggplot}
## map_data is a ggplot function! 
(bg.coast <- map_data("worldHires", "Bulgaria", xlim=c(27.0, 29.0), ylim = c(42.0, 43.0))
)

ggplot(bg.coast, aes(x = long, y = lat)) + 
  geom_polygon()
```
Cool!  

* read in a shapefile (from GIS_Ecology_BG; file(s) BG_Border copied into the data directory before starting). This results in a spdf object (= spatial polygons data frame).    
```{r background_map_shp}
## this needs library rgdal
bg_spdf <- readOGR(dsn = here(data.dir, "currents"), layer = "BG_Border")

## reproject to latitude/longitude - the original shapefile is in UTM, zone 35 N (these functions come from package sp)
bg_spdf_geog <- spTransform(bg_spdf, CRS("+proj=longlat +datum=WGS84"))

## plot to see if everything went fine..
plot(bg_spdf_geog)

## The plot doesn't show up in the notebook - probably something to do with sp's plotting method and knitr. It looks fine when run from the normal console/graphical device.
```

The spdf needs to be transformed into a dataframe that ggplot understands... Excellent overview & description of how and why here: https://github.com/tidyverse/ggplot2/wiki/plotting-polygon-shapefiles
```{r spdf_to_df}
bg_df <- fortify(bg_spdf_geog, region = "STATE_B_ID") ## region is the name of the id field of the data slot in the spdf

## plot to check things out.. 
ggplot(bg_df, aes(x = long, y = lat, group = group)) + 
  geom_polygon()
```
Zoom in on the southern Black Sea coast (same extent as the currents data from the Black Sea model)..  
```{r zoom_bg_coast}
ggplot(bg_df, aes(x = long, y = lat, group = group)) + 
  geom_polygon() + 
  coord_fixed(xlim = c(27, 29),  ylim = c(42, 43), ratio = 1.3)
```
Goody! Maybe remove the gridlines in the finished plot, though.  

#### Plot currents - test
Now that we have a background map, let's try plotting a subset of the currents..  
```{r test_plot_currents_sub}
## get only one date from the currents data frame for testing 
dummy.currents <- currents.all %>% 
  filter(time == as.POSIXct("2012-01-01", tz = "UTC")) 

ggplot() +
  ## plot the currents (filtered a bit to avoid overplotting)
  geom_segment(data = dummy.currents %>% filter(row_number() %% 2 == 0), aes(x = lon, y = lat, xend = lon + u * 0.25, yend = lat + v * 0.25),
arrow = arrow(angle = 15, length = unit(0.02, "inches"), type = "closed"), alpha = 0.3) + 
  ## plot the coastline
  geom_polygon(data = bg_df, aes(x = long, y = lat, group = group)) +
  ## zoom in on the coastline (keeping the aspect ratio)
  coord_fixed(xlim = c(27, 29),  ylim = c(42, 43), ratio = 1.3)
```
The currents data frame contains a lot of missing values which are automatically removed by ggplot - this causes the warning, but it's not a big deal.  


#### Average currents - by year  
I'm going to average the currents by year, for each latitude-longitude pair.  
Hopefully this produces something valid - I'm not really sure you can treat reanalysis data this way, but ok..  
```{r yearly_average_currents}
## make a copy of the currents data frame, which will be overwritten with the summary
currents.all.average <- currents.all

## add a column with the year to the summary currents data frame - will be used for grouping
(currents.all.average <- currents.all.average %>% 
    mutate(year = lubridate::year(time)) %>% 
    select(lon, lat, time, year, u, v) ## rearrange the columns a bit 
)

## for each longitude and latitude, summarize the currents by year
(currents.all.average <- currents.all.average %>%
    group_by(lon, lat, year) %>%
    transmute(u_mean = mean(u, na.rm = TRUE), v_mean = mean(v, na.rm = TRUE))
)
```

Plot the yearly averages..   
```{r plot_yearly_average_currents}
ggplot() +
  ## plot the currents (filtered a bit to avoid overplotting)
  geom_segment(data = currents.all.average %>% filter(dplyr::row_number() %% 2 == 0) %>% unique(), aes(x = lon, y = lat, xend = lon + u_mean * 0.5, yend = lat + v_mean * 0.5), ## higher scalar to (hopefully) make vectors more visible
arrow = arrow(angle = 15, length = unit(0.02, "inches"), type = "closed"), alpha = 0.3) + 
  facet_wrap(~ year) + ## facet by year
  ## plot the coastline
  geom_polygon(data = bg_df, aes(x = long, y = lat, group = group)) +
  ## zoom in on the coastline (keeping the aspect ratio)
  coord_fixed(xlim = c(27, 29),  ylim = c(42, 43), ratio = 1.3)
```
That's good, but even with the reduced dataset and the higher multiplication factor for the vectors the maps are practically unreadable..  

First try to fix this: plot the years one by one, then combine in one plot if needed.   
